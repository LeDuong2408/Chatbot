1. My Achievements:
- The model is deployed by integrating LangChain’s AddRoute with FastAPI. 
- The chatbot model can extract information from PDF files (academic papers). When users ask questions, the LLM model (Llama-2-7B-Chat.Q6_K) can provide accurate answers.
  
**Below are some examples:**

  _a. Example 1:_
  
  Q: What is OCR? 
  
  A: "OCR stands for Optical Character Recognition which is the process of converting printed or visual text into editable machine-encoded text. This process involves two main modules - text detection and text recognition. The current state-of-the-art models on printed, handwritten, and scene text recognition tasks are available publicly through the link mentioned in the content."
  
  ![1](https://github.com/user-attachments/assets/e05828ce-41f9-4769-a672-aca76ca01cbb)
  _b. Example 2:_
  
  Q: What is Attention?
  
  A: "Attention is a mechanism used to relate different positions in a sequence of data. It allows a model to weigh the importance of different parts of the input when computing its output. In the context of the Transformer model, self-attention is used to compute the weighted sum of the encoder's output, where the weights are learned during training."
  
  ![2](https://github.com/user-attachments/assets/46545471-a151-4ec8-bc07-aede9643784e)
  
  c. Example 3:
  
  Q: Does Transformer have parallel computing capabilities?
  
  A: "Yes, Transformer has parallel computing capabilities. According to the context, the Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs."
  
  ![3](https://github.com/user-attachments/assets/53078fc4-8a13-402b-a4c2-e63f1cff5dec)
  
2. Hướng dẫn cài đặt:
